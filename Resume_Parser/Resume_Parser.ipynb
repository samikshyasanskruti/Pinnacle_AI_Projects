{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMueJbmxBW59n+4wSEx+eEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samikshyasanskruti/Pinnacle_AI_Projects/blob/main/Resume_Parser/Resume_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pdfplumber python-docx spacy nltk python-magic\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KAJM9brm_uD",
        "outputId": "8507237b-4848-416a-91fb-6036c6d7599b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: python-magic, python-docx\n",
            "Successfully installed python-docx-1.2.0 python-magic-0.4.27\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os, json, pdfplumber, docx\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract text from PDF/DOCX\n",
        "def extract_text(file_path):\n",
        "    text = \"\"\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "    elif file_path.endswith(\".docx\"):\n",
        "        text = docx2txt.process(file_path)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "    return text\n",
        "\n",
        "# Skills database (expandable)\n",
        "SKILLS_DB = {\"python\",\"java\",\"c++\",\"c#\",\"javascript\",\"typescript\",\"react\",\n",
        "             \"angular\",\"nodejs\",\"express\",\"flask\",\"django\",\"pandas\",\"numpy\",\n",
        "             \"scikit-learn\",\"tensorflow\",\"keras\",\"pytorch\",\"machine learning\",\n",
        "             \"deep learning\",\"nlp\",\"computer vision\",\"sql\",\"mysql\",\"postgresql\",\n",
        "             \"mongodb\",\"docker\",\"kubernetes\",\"git\",\"html\",\"css\",\"rest\",\"api\",\n",
        "             \"aws\",\"azure\",\"gcp\",\"linux\",\"bash\",\"shell\",\"spark\",\"hadoop\",\n",
        "             \"opencv\",\"matplotlib\",\"seaborn\",\"tableau\",\"excel\"}\n",
        "\n",
        "EDU_KEYWORDS = [\"bachelor\",\"master\",\"b.sc\",\"b.tech\",\"m.tech\",\"msc\",\"mba\",\"phd\",\n",
        "                \"high school\",\"diploma\",\"university\",\"college\",\"institute\"]\n",
        "\n",
        "PHONE_REGEX = re.compile(r'(\\+?\\d{1,3}[\\s-])?(?:\\(?\\d{2,4}\\)?[\\s-]?)?\\d{6,12}')\n",
        "EMAIL_REGEX = re.compile(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+')\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    text_pages = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text_pages.append(page.extract_text() or \"\")\n",
        "    return \"\\n\".join(text_pages)\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "def extract_raw_text(path):\n",
        "    if path.lower().endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(path)\n",
        "    elif path.lower().endswith(\".docx\"):\n",
        "        return extract_text_from_docx(path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "def extract_emails(text):\n",
        "    return list({m.group(0) for m in EMAIL_REGEX.finditer(text)})\n",
        "\n",
        "def extract_phones(text):\n",
        "    phones = []\n",
        "    for m in PHONE_REGEX.finditer(text):\n",
        "        s = m.group(0)\n",
        "        digits = re.sub(r'\\D', '', s)\n",
        "        if 8 <= len(digits) <= 15:\n",
        "            phones.append(s.strip())\n",
        "    return list(dict.fromkeys(phones))\n",
        "\n",
        "def extract_name(nlp_doc, text):\n",
        "    sample = text[:1500]\n",
        "    doc = nlp(sample)\n",
        "    persons = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
        "    if persons:\n",
        "        return persons[0]\n",
        "    return text.splitlines()[0].strip()\n",
        "\n",
        "def extract_skills(text):\n",
        "    found = {s for s in SKILLS_DB if s.lower() in text.lower()}\n",
        "    return sorted(found)\n",
        "\n",
        "def extract_education(text):\n",
        "    edu = []\n",
        "    for s in sent_tokenize(text):\n",
        "        if any(k in s.lower() for k in EDU_KEYWORDS):\n",
        "            edu.append(s.strip())\n",
        "    return list(dict.fromkeys(edu))\n",
        "\n",
        "def extract_experience(text):\n",
        "    exp = []\n",
        "    for line in text.splitlines():\n",
        "        if \"experience\" in line.lower() or \"worked\" in line.lower() or \"intern\" in line.lower():\n",
        "            exp.append(line.strip())\n",
        "    return exp[:10]\n",
        "\n",
        "def parse_resume(path):\n",
        "    text = extract_raw_text(path)\n",
        "    result = defaultdict(lambda: None)\n",
        "    result[\"raw_text\"] = text\n",
        "    result[\"emails\"] = extract_emails(text)\n",
        "    result[\"phones\"] = extract_phones(text)\n",
        "    result[\"name\"] = extract_name(nlp, text)\n",
        "    result[\"skills\"] = extract_skills(text)\n",
        "    result[\"education\"] = extract_education(text)\n",
        "    result[\"experience\"] = extract_experience(text)\n",
        "    return result"
      ],
      "metadata": {
        "id": "u5g-fKSfnZ9D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed = parse_resume(resume_path)\n",
        "\n",
        "print(\"üéØ Extracted Resume Information:\\n\")\n",
        "print(json.dumps(parsed, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngZ4zErnnjZa",
        "outputId": "6712c3db-0240-4671-ca35-500a8581609d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Extracted Resume Information:\n",
            "\n",
            "{\n",
            "  \"raw_text\": \"Samikshya Sanskruti Swain\\nEmail: samikshya3009@gmail.com | Phone: +91-8926148570\\nGitHub: github.com/samikshyasanskruti | LinkedIn:\\nlinkedin.com/in/samikshya-sanskruti-swain\\nProfessional Summary\\nAspiring Computer Science Engineer with hands-on experience in deep\\nlearning, computer vision, web development, and game design. Passionate\\nabout building intelligent systems and interactive software. Enthusiastic\\ncontributor to open-source communities like GSSoC.\\nEducation\\nBachelor of Technology (B.Tech) ‚Äì Computer Science Engineering\\nSiksha ‚ÄòO‚Äô Anusandhan University, Bhubaneswar\\nSeptember 2023 ‚Äì June 2027 (Expected)\\nTechnical Skills\\n‚Ä¢ Languages: Python, Java, C++, JavaScript, Assembly\\n‚Ä¢ Web Technologies: HTML, CSS, React\\n‚Ä¢ Databases: MySQL\\n‚Ä¢ Libraries & Frameworks: NumPy, Pandas, Scikit-learn, Matplotlib\\n‚Ä¢ Tools & Platforms: Git, GitHub, VS Code, Jupyter Notebook, Tableau,\\nGodot Engine, EMU8086\\n‚Ä¢ Others: OpenCV, MediaPipe, Flask\\nProjects\\nHuman Activity Recognition\\nPython, NumPy, Pandas, Scikit-learn, Matplotlib\\n‚Ä¢ Developed a hybrid CNN-BiLSTM model to classify six types of human\\nactivity using the UCI HAR dataset.\\n‚Ä¢ Applied dropout regularization and batch normalization to optimize\\nperformance.\\n‚Ä¢ Evaluated model with classification report, confusion matrix, ROC-AUC\\ncurve, and accuracy/loss plots.\\n‚Ä¢ GitHub: Human Activity Recognition Project\\nEmotion Detection System\\nPython, HTML, CSS\\n‚Ä¢ Developed a real-time emotion detection web app using deep learning\\nand OpenCV.\\n‚Ä¢ Integrated with Flask backend and built a responsive front-end UI.\\n‚Ä¢ Recognizes multiple facial emotions from live webcam input.\\nVirtual Computer\\nPython, NumPy, OpenCV, MediaPipe\\n‚Ä¢ Built a hand-gesture-based virtual computer interface.\\n‚Ä¢ Enabled gesture-based interaction with a virtual mouse and keyboard.\\nDSA Chatbot\\nHTML, CSS, JavaScript\\n‚Ä¢ Created a chatbot to help users learn and revise Data Structures &\\nAlgorithms.\\n‚Ä¢ Designed a clean front-end UI and implemented response logic with\\nJavaScript.\\nAmazon Clone Website\\nHTML, CSS\\n‚Ä¢ Developed a pixel-perfect front-end clone of the Amazon homepage.\\n‚Ä¢ Ensured responsive design and clean layout with semantic HTML/CSS.\\nDemon Slayer 2D Game\\nGodot Engine, GDScript\\n‚Ä¢ Designed a 2D platformer inspired by ‚ÄúDemon Slayer‚Äù anime.\\n‚Ä¢ Added mechanics like enemy AI, scoring, and character animations.\\nBalloon Blaster Game\\nAssembly Language, EMU8086\\n‚Ä¢ Programmed a 2D shooting game in x86 Assembly.\\n‚Ä¢ Used interrupts for graphics, bullet logic, and control mechanisms.\\n‚Ä¢ GitHub: Balloon Game\\nAI Trading Bot (In Progress)\\nPython, Machine Learning, Pandas, NumPy, yFinance API\\n‚Ä¢ Designing an AI-based bot to analyze market trends and make trading\\ndecisions.\\n‚Ä¢ Working on predictive modeling, backtesting, and real-time stock\\nmonitoring.\\nCertifications\\n‚Ä¢ Deloitte Australia Cyber Job Simulation ‚Äì Forage (July 2025): Simulated\\nincident response, log investigation, and suspicious activity analysis.\\n‚Ä¢ Deloitte Australia Data Analytics Simulation ‚Äì Forage (July 2025): Built a\\nTableau dashboard; analyzed business data using Excel.\\n‚Ä¢ Deloitte Australia Technology Simulation ‚Äì Forage (July 2025): Proposed\\na dashboard app design as part of a virtual product development team.\\nAchievements & Community Contributions\\n‚Ä¢ GSSoC ‚Äô25 Contributor ‚Äì Actively contributed to open-source projects\\nduring GirlScript Summer of Code 2025.\",\n",
            "  \"emails\": [\n",
            "    \"samikshya3009@gmail.com\"\n",
            "  ],\n",
            "  \"phones\": [\n",
            "    \"+91-8926148570\"\n",
            "  ],\n",
            "  \"name\": \"Samikshya Sanskruti Swain\\nEmail\",\n",
            "  \"skills\": [\n",
            "    \"api\",\n",
            "    \"c++\",\n",
            "    \"computer vision\",\n",
            "    \"css\",\n",
            "    \"deep learning\",\n",
            "    \"excel\",\n",
            "    \"flask\",\n",
            "    \"git\",\n",
            "    \"html\",\n",
            "    \"java\",\n",
            "    \"javascript\",\n",
            "    \"machine learning\",\n",
            "    \"matplotlib\",\n",
            "    \"mysql\",\n",
            "    \"numpy\",\n",
            "    \"opencv\",\n",
            "    \"pandas\",\n",
            "    \"python\",\n",
            "    \"react\",\n",
            "    \"scikit-learn\",\n",
            "    \"sql\",\n",
            "    \"tableau\"\n",
            "  ],\n",
            "  \"education\": [\n",
            "    \"Education\\nBachelor of Technology (B.Tech) ‚Äì Computer Science Engineering\\nSiksha ‚ÄòO‚Äô Anusandhan University, Bhubaneswar\\nSeptember 2023 ‚Äì June 2027 (Expected)\\nTechnical Skills\\n‚Ä¢ Languages: Python, Java, C++, JavaScript, Assembly\\n‚Ä¢ Web Technologies: HTML, CSS, React\\n‚Ä¢ Databases: MySQL\\n‚Ä¢ Libraries & Frameworks: NumPy, Pandas, Scikit-learn, Matplotlib\\n‚Ä¢ Tools & Platforms: Git, GitHub, VS Code, Jupyter Notebook, Tableau,\\nGodot Engine, EMU8086\\n‚Ä¢ Others: OpenCV, MediaPipe, Flask\\nProjects\\nHuman Activity Recognition\\nPython, NumPy, Pandas, Scikit-learn, Matplotlib\\n‚Ä¢ Developed a hybrid CNN-BiLSTM model to classify six types of human\\nactivity using the UCI HAR dataset.\"\n",
            "  ],\n",
            "  \"experience\": [\n",
            "    \"Aspiring Computer Science Engineer with hands-on experience in deep\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    resumes = [\"resume1.pdf\", \"resume2.docx\"]  # put your files here\n",
        "    results = []\n",
        "    for r in resumes:\n",
        "        data = parse_resume(r)\n",
        "        results.append(data)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"parsed_resumes.csv\", index=False)\n",
        "    print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM3cKN-Jpi7X",
        "outputId": "b6ff44b2-f889-4783-d33a-3ace36490660"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            raw_text  \\\n",
            "0  Samikshya Sanskruti Swain\\nEmail: samikshya300...   \n",
            "1  Kunal Routray\\nEmail: kunalroutray91221@gmail....   \n",
            "\n",
            "                          emails                      phones  \\\n",
            "0      [samikshya3009@gmail.com]            [+91-8926148570]   \n",
            "1  [kunalroutray91221@gmail.com]  [+91-7008602621, 95721128]   \n",
            "\n",
            "                               name  \\\n",
            "0  Samikshya Sanskruti Swain\\nEmail   \n",
            "1                       Bhubaneswar   \n",
            "\n",
            "                                              skills  \\\n",
            "0  [api, c++, computer vision, css, deep learning...   \n",
            "1  [api, c++, computer vision, css, deep learning...   \n",
            "\n",
            "                                           education  \\\n",
            "0  [Education\\nBachelor of Technology (B.Tech) ‚Äì ...   \n",
            "1  [Education \\nBachelor of Technology (B.Tech) ‚Äì...   \n",
            "\n",
            "                                          experience  \n",
            "0  [Aspiring Computer Science Engineer with hands...  \n",
            "1  [Aspiring Computer Science Engineer with hands...  \n"
          ]
        }
      ]
    }
  ]
}